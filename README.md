# üöÄ Semantic Search: FastText Model 

Dieses Projekt automatisiert die Erstellung eines dom√§nenspezifischen Wort-Vektor-Modells (100-dimensional) f√ºr den RWA-Produktkatalog. Es kombiniert das linguistische Wissen von Facebooks FastText mit den spezifischen Produktdaten aus dem Solr-Index.

---

## üõ† 1. System-Voraussetzungen & Setup

### Python-Umgebung
Das Projekt wurde f√ºr **Python 3.9+** optimiert. Um eine saubere Ausf√ºhrung zu gew√§hrleisten, sollte ein lokaler Interpreter (oder eine Virtual Environment) verwendet werden.

### Erforderliche System-Tools
Stelle sicher, dass `curl` und `gzip` auf deinem System verf√ºgbar sind (auf macOS Standard).

### Installation der Python-Bibliotheken
F√ºhre den folgenden Befehl in deinem Terminal aus, um alle notwendigen Abh√§ngigkeiten f√ºr das Training, die API-Kommunikation und die Visualisierungen zu installieren:

```bash
python3.9 -m pip install requests numpy pandas matplotlib seaborn scikit-learn fasttext-wheel
```

| Bibliothek | Zweck |
| :--- | :--- |
| **fasttext-wheel** | Die Core-Engine zum Laden, Reduzieren und Trainieren der Modelle. |
| **requests** | Erm√∂glicht den Datenabruf vom RWA-Solr-Server. |
| **scikit-learn** | Berechnet t-SNE Projektionen und KMeans-Cluster f√ºr die Visualisierung. |
| **matplotlib / seaborn** | Generiert die Heatmaps und semantischen Netzwerk-Grafiken. |
| **numpy / pandas** | Mathematische Operationen und effiziente Datenstrukturierung. |

---

## üìà 2. Workflow & Skript-Reihenfolge

Um das semantische Modell korrekt aufzubauen, m√ºssen die Skripte in der folgenden numerischen Reihenfolge ausgef√ºhrt werden. Jedes Skript baut auf dem Output des vorherigen auf.

### 1Ô∏è‚É£ Schritt 1: Basis-Modell vorbereiten (`download_base.py`)
Dieses Skript stellt sicher, dass das fundamentale Sprachwissen vorhanden ist.
* **Was es tut:** L√§dt das deutsche Facebook FastText-Modell (~4.2GB) herunter und entpackt es.
* **Output:** `cc.de.300.bin` im Hauptverzeichnis.
* **Wichtig:** Das Skript l√∂scht die `.gz` Datei nach dem Entpacken automatisch, um Speicherplatz zu sparen.

### 2Ô∏è‚É£ Schritt 2: Rohdaten-Extraktion (`fetch_solr_data.py`)
Verbindet sich mit dem RWA-Solr-Index, um die Produktinformationen zu exportieren.
* **Was es tut:** Ruft ca. 163.000 Dokumente ab (Namen, Marken, SEO-Texte).
* **Output:** `fasttext_project/domain_corpus.txt`.
* **Hinweis:** F√ºhrt eine erste Wort-Deduplizierung innerhalb jeder Zeile durch.

### 3Ô∏è‚É£ Schritt 3: Intensiv-Reinigung (`sanitize_corpus.py`)
Verwandelt den unstrukturierten Text in hochwertiges Trainingsmaterial.
* **Was es tut:** * Entfernt Stoppw√∂rter und Sonderzeichen.
    * F√ºgt Ma√üeinheiten zusammen (z.B. `10 kg` ‚Üí `10kg`).
    * Normalisiert Dimensionen (z.B. `40 x 40` ‚Üí `40x40`).
* **Output:** `domain_corpus_clean.txt`.

### 4Ô∏è‚É£ Schritt 4: Modell-Training & Transfer Learning (`train_semantic_model.py`)
Die eigentliche Erzeugung des k√ºnstlichen neuronalen Netzes.
* **Was es tut:** 1. Reduziert das Facebook-Modell auf **100 Dimensionen** (f√ºr effiziente RAM-Nutzung).
    2. Trainiert ein `skipgram`-Modell mit deinem Korpus unter Nutzung von **Pretrained Vectors**.
    3. Nutzt n-grams (3-6), damit auch Teilw√∂rter und Tippfehler erkannt werden.
* **Output:** `rwa_semantic_model_100d.bin`.

---

## üîç 4. Validierung & Visualisierung

Nachdem das Modell trainiert wurde, stehen verschiedene Tools zur Verf√ºgung, um die Qualit√§t der gelernten Vektoren zu pr√ºfen und die semantischen Beziehungen grafisch darzustellen.

### üõ† √Ñhnlichkeits-Check (`compare_similarity.py`)
Dieses Skript dient der mathematischen √úberpr√ºfung deines Modells.
* **Funktion:** Berechnet die **Cosine-Similarity** (Werte von 0.0 bis 1.0) zwischen zwei beliebigen Begriffen oder ganzen S√§tzen.
* **Anwendung:** Ideal, um zu testen, wie stark das Modell Fachbegriffe (z.B. "Granitplatte") mit ihren Attributen (z.B. "Naturstein") verkn√ºpft.

### üå° Vektor-Heatmap (`visualize_word_heatmap.py`)
Erzeugt eine visuelle Matrix der Wort-Vektoren.
* **Output:** `rwa_model_heatmap.png`
* **Details:** Visualisiert die Top-50 W√∂rter deines Katalogs. Jede Zeile repr√§sentiert ein Wort, jede Spalte eine der 100 Dimensionen. √Ñhnliche Farbmuster √ºber die Zeilen hinweg zeigen an, dass das Modell diese W√∂rter als semantisch verwandt eingestuft hat.

**Beispiel-Visualisierung:**
![RWA Word Heatmap](rwa_model_heatmap.png)

### üåê Semantisches Netzwerk (`visualize_semantic_network.py`)
Erstellt eine Cluster-Karte basierend auf dem t-SNE Algorithmus.
* **Output:** `connected_bubble_map.png`
* **Details:** Projiziert die 100-dimensionalen Vektoren auf eine 2D-Fl√§che.
  * **Cluster:** W√∂rter werden automatisch in Gruppen (z.B. "Werkzeug", "Garten", "Baustoffe") farblich markiert.
  * **Bubbles:** Die Gr√∂√üe der Kreise zeigt die H√§ufigkeit der W√∂rter im Korpus.
  * **Verbindungen:** Linien/Pfeile zeigen die st√§rksten semantischen "Anziehungskr√§fte" zwischen Begriffen.

**Beispiel-Visualisierung:**
![Connected Semantic Bubble Map](connected_bubble_map.png)

---